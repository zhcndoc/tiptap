---
title: 集成自定义后端和 LLM
meta:
  title: 自定义 LLM | Tiptap AI 建议
  description: 使用 AI 建议扩展集成您自己的后端和 LLM 以获得自定义建议。
  category: 内容 AI
---

import { Callout } from '@/components/ui/Callout'

默认情况下，AI 建议扩展使用 Tiptap 内容 AI 云生成您的内容的建议。这使您能够以最少的设置使用其功能。然而，您可以集成自己后端和 LLM 以生成建议。

<Callout title="自定义 LLM 演示" variant="info">
  我们为我们的商业和企业客户提供详细的自定义 LLM 演示。它包括客户端和服务器代码，以及如何运行和部署的说明。请参见 [计划和定价](https://tiptap.dev/pricing)。
</Callout>

AI 建议扩展支持不同程度的自定义。您可以：

1. 使用 Tiptap 内容 AI 云，但自定义 OpenAI 模型。
2. 用您自己的 LLM 和后端替换获取建议数据的 API 端点，但让扩展处理建议的显示和应用。这是大多数用例推荐的方法，因为我们为您处理了大部分复杂性：比较旧的和新的编辑器内容，以愉快的方式显示差异，并处理冲突。
3. 完全实现您自己的解析器函数。这使您能够完全控制如何在编辑器中显示建议。只有在高级场景中推荐这种方法。

## 在 Tiptap Cloud 中自定义 OpenAI 模型

您可以使用 `model` 选项配置用于生成建议的 OpenAI 模型。默认模型是 `gpt-4o-mini`。它在速度、成本和准确性之间提供了良好的平衡。

如果您希望提高建议的质量，可以使用更大的模型如 `gpt-4o`。请注意，更大的模型通常会更昂贵、更慢，并且延迟更高。

```ts
AiSuggestion.configure({
  // 用于生成建议的模型。默认为 "gpt-4o-mini"
  model: 'gpt-4o',
})
```

## 替换 API 端点（推荐）

如果您想使用自己的后端和 LLM 生成建议，可以提供一个自定义的 `apiResolver` 函数。该函数应调用您的后端，并根据编辑器的内容和规则返回建议数组。

```tsx
AiSuggestion.configure({
  async resolver({ defaultResolver, ...options }) {
    const suggestions = defaultResolver({
      ...options,
      apiResolver: async ({ html, htmlChunks, rules }) => {
        // 通过调用您自定义的后端和 LLM 生成响应
        const response = await claudeSonnetApi({ html, htmlChunks, rules })

        // 以正确的格式返回响应（见下文细节）
        return { format: 'replacements', content: response }
      },
    })

    return suggestions
  },
})
```

为提供最大的灵活性，`apiResolver` 接受两种格式的响应：

### `replacements` 格式（推荐）

响应是一个替换数组，将应用于编辑器的内容。当建议修改文档的特定部分时，此格式非常有用。它是我们 Tiptap 内容 AI 云内部使用的格式，迄今为止为我们提供了最佳结果。
它包含以下属性：

- `items`: 建议列表。每个建议具有以下属性：
  - `ruleId`: 生成建议的规则 ID。
  - `deleteHtml`: 要从编辑器中删除的 HTML 内容。
  - `insertHtml`: 要插入到编辑器中的 HTML 内容。
  - `chunkId`: 建议应应用的块 ID。
- `htmlChunks`: 编辑器内容中所有 HTML 代码块的列表。每个块具有以下属性：
  - `id`: 块的唯一标识符。
  - `html`: 块的 HTML 内容。
  - `generateSuggestions`: AI 模型是否为此块生成了建议。否则，如果设置为 `false`，则建议将从扩展的内部缓存中获取。

确保 `htmlChunks` 属性包含编辑器内容中的所有 HTML 代码块，否则缓存机制将无法正常工作。

您可以通过读取 `apiResolver` 函数的 `htmlChunks` 参数轻松获取 `htmlChunks` 属性的值。

以下是用 `replacements` 格式的示例响应。

```json
{
  "format": "replacements",
  "content": {
    "htmlChunks": [
      {
        "id": "1",
        "html": "<p>你好，欢迎使用我们的超棒应用！我们希望你们会喜欢它。我们的应用程序提供独特的功能，增强您的烹饪体验。您可以探索各种美食并分享您的美食时刻。</p><p>Hola, estamos emocionados de tenerte aquí. 我们的应用程序不仅仅是关于食谱，而是关于建立社区。我们相信这将改变您的烹饪方式。</p>",
        "generateSuggestions": true
      },
      {
        "id": "2",
        "html": "<p>请查看我们的酷功能，享受与我们一起烹饪。如果您有疑问，请随时提问。</p>",
        "generateSuggestions": true
      }
    ],
    "items": [
      {
        "ruleId": "1",
        "deleteHtml": "Hola, estamos <bold>emocionados</bold> de tenerte aquí.",
        "insertHtml": "你好，我们很<bold>兴奋</bold>你在这里。",
        "chunkId": "1"
      },
      {
        "ruleId": "2",
        "deleteHtml": "aplication",
        "insertHtml": "application",
        "chunkId": "1"
      },
      {
        "ruleId": "2",
        "deleteHtml": "momentts",
        "insertHtml": "moments",
        "chunkId": "1"
      },
      {
        "ruleId": "1",
        "deleteHtml": "Si tienes dudas, no dudes en preguntar.",
        "insertHtml": "如果您有任何问题，请随时问。",
        "chunkId": "2"
      },
      {
        "ruleId": "2",
        "deleteHtml": "fetures",
        "insertHtml": "features",
        "chunkId": "2"
      }
    ]
  }
}
```

### `fullHtml` 格式

响应是一个完整的 HTML 字符串，将替换编辑器的内容。当您希望用建议替换整个内容时，此格式非常有用。我们发现这个格式在只要应用一个规则时性能非常好，但在需要多个规则时效果较差。此格式不支持分块和缓存。

```json
{
  "format": "fullHtml",
  "content": {
    "items": [
      {
        "ruleId": "1",
        "fullHtml": "<p>你好，欢迎使用我们的超棒应用！我们希望你们会喜欢它。我们的应用程序提供独特的功能，增强您的烹饪体验。您可以探索各种美食并分享您的美食时刻。</p><p>你好，我们很兴奋你在这里。我们的应用程序不仅仅是关于食谱，而是关于建立社区。我们相信这将改变您的烹饪方式。</p><p>请查看我们的酷功能，享受与我们一起烹饪。如果您有疑问，请随时提问。</p>"
      },
      {
        "ruleId": "2",
        "fullHtml": "<p>你好，欢迎使用我们的超棒应用！我们希望你们会喜欢它。我们的应用程序提供独特的功能，增强您的烹饪体验。您可以探索各种美食并分享您的美食时刻。</p><p>Hola, estamos emocionados de tenerte aquí. 我们的应用程序不仅仅是关于食谱，而是关于建立社区。我们相信这将改变您的烹饪方式。</p><p>请查看我们的酷功能，享受与我们一起烹饪。如果您有疑问，请随时提问。</p>"
      }
    ]
  }
}
```

### 提高准确性

LLM 可能会犯错误，这使得确保其响应遵循所需格式变得困难。为了提高自定义模型的准确性，我们建议遵循以下最佳实践和提示工程技术。

- 在实施改进之前，评估您自定义端点的响应并测量其性能和准确性。使用像 [Evalite](https://www.evalite.dev/) 这样的评估框架。这有助于您细化和比较提示以获得更好的性能。
- 如果您使用 `replacements` 格式，请利用 [OpenAI 结构化输出](https://platform.openai.com/docs/guides/structured-outputs)（或其他 LLM 提供商的等效功能）以确保响应是符合特定架构的 JSON 对象。
- 如果您使用 `fullHtml` 格式，可以利用 [OpenAI 预测输出](https://platform.openai.com/docs/guides/predicted-outputs)（或其他 LLM 提供商的等效功能）以确保响应不会与原始编辑器内容偏离过大。
- 调整 `temperature` 或 `top_p` [参数](https://platform.openai.com/docs/api-reference/completions/create) 来控制模型输出的随机性。
- 各个 LLM 提供商都有官方的 [最佳实践](https://platform.openai.com/docs/guides/prompt-engineering)、优化 [延迟](https://platform.openai.com/docs/guides/latency-optimization) 和提高 [准确性](https://platform.openai.com/docs/guides/optimizing-llm-accuracy) 的指南。
- 不同的校对规则在不同的提示方法和响应格式中效果最佳。您不必在 `replacements` 或 `fullHtml` 格式之间进行选择——您可以两者都使用！定义一个返回 `replacements` 格式建议的 API 端点，另一个生成 `fullHtml` 格式的建议。以下是一个示例：

  ```tsx
  AiSuggestion.configure({
    async resolver({ defaultResolver, rules, ...options }) {
      // 将规则分成两组
      const {
        rulesForFirstApi,
        rulesForSecondApi,
      } = splitRules(rules)

      // 将第一组规则发送到第一个 api 端点
      const suggestions1 = await defaultResolver({
        ...options,
        rules: rulesForFirstApiEndpoint,
        apiResolver: async ({ html, htmlChunks, rules }) => {
          const response = await firstApi({ html, htmlChunks, rules });
          return { format: "replacements", content: response };
        },
      });

      // 将第二组规则发送到第二个 api 端点
      const suggestions2 = await defaultResolver({
        ...options,
        rules: rulesForSecondApiEndpoint,
        apiResolver: async ({ html, htmlChunks, rules }) => {
          const response = await secondApi({ html, htmlChunks, rules });
          return { format: "fullHtml", content: response };
        },
      });

      // 合并两个建议列表
      return [...suggestions1, ...suggestions2]
    },
  })
  ```

### 提高性能

为了提高您的 API 和自定义 LLM 模型在生成建议时的性能，请考虑以下策略：

- 选择一种在准确性、速度和低延迟之间取得平衡的模型。使用像 [Artificial Analysis](https://artificialanalysis.ai/#intelligence-vs-output-speed) 这样的排行榜来比较不同指标下的模型。
- 在实施性能改进之前，测量您的 API 响应时间。如果您的管道包含多个步骤或模型调用，请分析每个步骤以识别瓶颈。使用像 [Evalite](https://www.evalite.dev/) 这样的评估框架迭代提示并比较其有效性。
- 利用 [OpenAI 预测输出](https://platform.openai.com/docs/guides/predicted-outputs)（或其他 LLM 提供商的等效功能）来提高延迟和处理速度。
- 将编辑器内容分块处理，并并行处理以减少总体处理时间，特别是对于大型文档。利用扩展的内置分块和缓存机制，仅重新加载已更改的块的建议。
- 结构化您的提示以充分利用完整或 [部分缓存](https://platform.openai.com/docs/guides/prompt-caching)。注意，不同 LLM 提供商提供不同的缓存实现，有些可能不支持。如果必要，可以实现您自己的缓存机制，以便对相同或相似的提示重用 LLM 响应。
- 流式响应可以提高您 API 的感知性能，特别是对于大型文档，通过允许在生成完整响应之前显示建议。虽然我们的 AI 建议扩展目前不支持流式处理，但我们正在考虑在未来版本中添加此功能。如果您认为您的应用程序会从中受益，请通过 humans@tiptap.dev 与我们联系。

## 完全替换解析器函数（高级）

如果您想完全控制如何生成编辑器建议，包括它们在文档中的准确位置，可以通过提供自定义的 `resolver` 函数来实现。该函数应根据编辑器的内容和规则返回建议数组。

要生成有效的建议对象，您的代码需要计算它们在编辑器中的 [位置](https://prosemirror.net/docs/guide/#doc.indexing)。这很可能涉及比较编辑器的当前内容与 LLM 生成的内容。要查看如何做到这一点的示例，您可以检查扩展源代码中的默认解析器函数。

有关每个建议对象应包含的数据的更多信息，请查看 [API 参考](/content-ai/capabilities/suggestion/api-reference#proofreading-suggestions)。

```tsx
AiSuggestion.configure({
  async resolver({ defaultResolver, ...options }) {
    const suggestions = await customResolver(options)
    return suggestions
  },
})
```

总体而言，实施自定义解析器的方法需要更多的工作，但它将为您提供更多的灵活性。我们只在高级用例中推荐它。

## 将 Tiptap 内容 AI 云与您的自定义后端结合起来

您不必在使用 Tiptap 内容 AI 云或您自己的后端之间进行选择。您可以将两者结合，以获得最佳效果。

```tsx
AiSuggestion.configure({
  async resolver({ defaultResolver, rules, ...options }) {
    // 将规则分成两组
    const { rulesForDefaultSuggestions, rulesForCustomSuggestions } = splitRules(rules)

    // 从 Tiptap 内容 AI 云获取建议
    const defaultSuggestions = await defaultResolver({
      ...options,
      rules: rulesForDefaultSuggestions,
    })
    // 从您自己的后端获取建议
    const customSuggestions = await customResolver({
      ...options,
      rules: rulesForCustomSuggestions,
    })

    // 合并两个建议列表
    return [...defaultSuggestions, ...customSuggestions]
  },
})
```

## 在没有 AI 的情况下生成校对建议

您并不需要使用 AI 来生成校对建议。您可以将 AI 模型与经典的校对技术结合起来。例如，您可以检查某些单词并进行替换。以下是一个生成将单词“hello”替换为“goodbye”的建议的解析器示例。

```tsx
AiSuggestion.configure({
  rules: [
    {
      id: '1',
      title: '将 hello 替换为 goodbye',
      // 该提示将不被使用，因为我们不使用 LLM 为此规则生成建议
      prompt: '将 hello 替换为 goodbye',
      color: '#DC143C',
      backgroundColor: 'FFE6E6',
    },
  ],
  async resolver({ defaultResolver, ...options }) {
    const suggestions = await defaultResolver({
      ...options,
      apiResolver: async ({ html, rules }) => {
        // 在不需要调用 LLM 的情况下生成响应
        return {
          format: 'fullHtml',
          content: {
            items: [
              {
                ruleId: '1',
                // 返回替换 "hello" 为 "goodbye" 后的新文档 HTML
                fullHtml: html.replaceAll('hello', 'goodbye'),
              },
            ],
          },
        }
      },
    })

    return suggestions
  },
})
```